{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1: Implementación de Red Neuronal sin librería:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #generar números aleatorios\n",
    "import numpy as np #para trabajar con arrays y matrices\n",
    "from keras.datasets import mnist #conjunto de datos de imágenes de dígitos escritos a mano\n",
    "from keras.utils import to_categorical #\n",
    "from sklearn.preprocessing import MinMaxScaler #scalar los datos a un rango\n",
    "from sklearn.model_selection import train_test_split #dividir los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "# Cargar el dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Se crea una lista de tuplas con los elementos de x e y, luego se toma una muestra aleatoria de 2000 tuplas, se descomprime y con zip se vuelven a separar en elementos de x e y \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Convertir en vector\n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1)) #Aplana las imágenes de 28x28 píxeles en un vector de 784 elementos (28*28 = 784)\n",
    "\n",
    "# Normalizamos Min-Max\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Dividimos la muestra en dos, una para entrenar y otra para testing, testeamos con la misma cantidad que entrenamos.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos para los modelos de clasificación que y_train sea un valor categórico\n",
    "y_train_value = y_train\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Clase base para las capas \n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    #espera a que las demas capas las implementen\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# Clase para capas densas (fully connected)\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0):\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg  #Regularización L2\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    #Actualiza los pesos usando el gradiente descendente\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Regularización L2 que castiga error en los pesos\n",
    "        weights_error += self.lambda_reg * self.weights\n",
    "\n",
    "        # Actualizar los parámetros\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "# Clase para Capa de Activación. Junto con la capa densa forman perceptrones, activación a la salida de una capa densa\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "    \n",
    "# Clase Red, conecta múltiples capas.\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # agrega una capa\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    #define la función de perdida y su derivada\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "    #predicción en datos de entrada pasando a través de las capas.\n",
    "    def predict(self, input_data):\n",
    "        if input_data.ndim == 1: \n",
    "            input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "        for i in range(samples):\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "    #entrenamiento de la red\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        if x_train[0].ndim == 1:\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "                err += self.loss(y_train[j], output) \n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "            err /= samples\n",
    "            \n",
    "            # calculamos el error promedio entre nodos de salida.\n",
    "            err = np.mean(err)\n",
    "            \n",
    "            # Imprimimos el error promedio de cada época, más que nada\n",
    "            # para seguimiento del aprendizaje. \n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "    # Funciones de Activación, y su correspondiente función derivada. \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)  # Calculamos sigmoid(x) para cada elemento del vector\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    # Versión aproximada de ReLu', porque ReLu no es redivable en x=0\n",
    "    # La derivada de ReLU es 1 si x > 0, y 0 si x <= 0\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# función Leaky ReLU\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # Si x es positivo, devuelve x; si no, devuelve alpha * x\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Implementamos la derivada de Leaky ReLU\n",
    "def leaky_relu_prime(x, alpha=0.01):\n",
    "    # La derivada es 1 si x > 0, y alpha si x <= 0\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "\n",
    "# Funciones de pérdida y su derivada. \n",
    "# Error Cuadrático\n",
    "def mse(y_true, y_hat):\n",
    "    return (y_true-y_hat)**2\n",
    "\n",
    "def mse_prime(y_true, y_hat):\n",
    "    return 2*(y_hat-y_true)\n",
    "\n",
    "#generador #que crea mini-batches que entrenan la red en pequeños subconjuntos de los datos.\n",
    "\n",
    "def mini_batch_generator(X, y, batch_size):\n",
    "    batch_size = 32\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices](y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "# Implementamos la función de error BCE\n",
    "def bce(y_true, y_hat):\n",
    "    # Evitamos problemas de logaritmo aplicando un pequeño epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "# Implementamos la derivada de BCE\n",
    "def bce_prime(y_true, y_hat):\n",
    "    # Evitamos problemas de división por 0\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Derivada de BCE con respecto a la predicción\n",
    "    return -(y_true / y_hat) + (1 - y_true) / (1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, batch_size=32):\n",
    "    n_samples, n_features = X.shape\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.bias = 0\n",
    "\n",
    "    for _ in range(self.n_iters):\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices) #baraja los indices de las muestras\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # Procesamiento del mini-batch\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Calculamos la salida del modelo, se multiplica el mini-batch por los pesos + el sesgo\n",
    "            linear_output = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_predicted = self._unit_step_function(linear_output)\n",
    "\n",
    "            # Calculamos el error\n",
    "            error = y_batch - y_predicted\n",
    "\n",
    "            # Calculamos el gradiente promedio del mini-batch\n",
    "            dw = np.mean(error * X_batch, axis=0)\n",
    "            db = np.mean(error)\n",
    "\n",
    "            # Realizamos el ajuste de pesos, con el gradiente calculado y el learning rate\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "        # Ajuste del último mini-batch, para que todas las muestras sean usadas\n",
    "        if i + batch_size > n_samples:\n",
    "            X_batch = X_shuffled[i:]\n",
    "            y_batch = y_shuffled[i:]\n",
    "\n",
    "            # Calculamos la salida\n",
    "            linear_output = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_predicted = self._unit_step_function(linear_output)\n",
    "\n",
    "            # Calculamos el error\n",
    "            error = y_batch - y_predicted\n",
    "\n",
    "            # Calculamos el gradiente del mini-batch\n",
    "            dw = np.mean(error * X_batch, axis=0)\n",
    "            db = np.mean(error)\n",
    "\n",
    "            # Ajuste de pesos\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce la dimensionalidad de las entradas tomando el valor máximo definidos en el pool_size.\n",
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        #El tamaño del pool, es decir, la dimensión de la ventana para realizar el max pooling\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        #Redimensionamos los datos de entrada (reshape) en bloques del tamaño de la ventana\n",
    "        self.output = np.max(input_data.reshape(-1, self.pool_size, self.pool_size), axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = np.zeros_like(self.input)\n",
    "        for i in range(self.pool_size):\n",
    "            for j in range(self.pool_size):\n",
    "                error[:, i::self.pool_size, j::self.pool_size] = output_error\n",
    "        return error\n",
    "#Reduce la dimensionalidad tomando el promedio de los valores de los bloques\n",
    "class AveragePoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.mean(input_data.reshape(-1, self.pool_size, self.pool_size), axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = np.zeros_like(self.input)\n",
    "        for i in range(self.pool_size):\n",
    "            for j in range(self.pool_size):\n",
    "                error[:, i::self.pool_size, j::self.pool_size] = output_error / (self.pool_size ** 2)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase NoiseLayer que implementa una capa que agrega ruido a los datos de entrada.\n",
    "# una pequeña dosis de caos que obliga al modelo a aprender de datos ligeramente diferentes cada vez, para evitar el sobreajuste\n",
    "class NoiseLayer(Layer):\n",
    "    def __init__(self, noise_std):\n",
    "        self.noise_std = noise_std #noise_std determina la cantidad de ruido que se agregará a los datos de entrada.\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = input_data + np.random.normal(0, self.noise_std, size=input_data.shape) #crea valores aleatorios alrededor de 0 con una desviación estándar de noise_std\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reduce sobreajuste al penalizar coeficientes, eliminando características irrelevantes automáticamente.\n",
    "class L1RegularizationLayer(Layer):\n",
    "    def __init__(self, lambda_reg):\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = input_data\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = output_error\n",
    "        for i in range(self.input.shape[1]):\n",
    "            #Si el valor de la entrada es positivo, se agrega lambda_reg al error\n",
    "            if self.input[0, i] > 0:\n",
    "                error[0, i] += self.lambda_reg\n",
    "            elif self.input[0, i] < 0:\n",
    "                error[0, i] -= self.lambda_reg #se resta lambda_reg del error\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.266525\n",
      "epoch 2/20   error=0.168264\n",
      "epoch 3/20   error=0.125138\n",
      "epoch 4/20   error=0.100078\n",
      "epoch 5/20   error=0.082120\n",
      "epoch 6/20   error=0.068167\n",
      "epoch 7/20   error=0.057069\n",
      "epoch 8/20   error=0.048194\n",
      "epoch 9/20   error=0.041070\n",
      "epoch 10/20   error=0.035277\n",
      "epoch 11/20   error=0.030589\n",
      "epoch 12/20   error=0.026726\n",
      "epoch 13/20   error=0.023521\n",
      "epoch 14/20   error=0.020862\n",
      "epoch 15/20   error=0.018652\n",
      "epoch 16/20   error=0.016792\n",
      "epoch 17/20   error=0.015209\n",
      "epoch 18/20   error=0.013832\n",
      "epoch 19/20   error=0.012623\n",
      "epoch 20/20   error=0.011543\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 88   0   0   1   1   2   2   1   1   0]\n",
      " [  1 117   0   1   0   2   1   0   0   0]\n",
      " [  0   1  80   3   1   1   4   1   2   1]\n",
      " [  0   0   5  94   1   5   0   3   5   2]\n",
      " [  1   0   1   0  84   2   1   2   2  10]\n",
      " [  1   0   0   0   1  81   0   1   2   0]\n",
      " [  2   0   3   0   0   1  90   0   3   0]\n",
      " [  2   0   1   0   1   0   0  81   1   0]\n",
      " [  1   4   1   6   2   4   0   0  81   2]\n",
      " [  1   1   1   1   4   1   0   4   0  85]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 128))\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(FCLayer(128, 64))\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(64, 10)) \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Asignamos función de pérdida\n",
    "model.use(bce, bce_prime)\n",
    "\n",
    "# Entrenamos el modelo con datos de entrenamiento\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.01)\n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.263571\n",
      "epoch 2/20   error=0.175913\n",
      "epoch 3/20   error=0.137749\n",
      "epoch 4/20   error=0.113862\n",
      "epoch 5/20   error=0.096847\n",
      "epoch 6/20   error=0.083976\n",
      "epoch 7/20   error=0.074251\n",
      "epoch 8/20   error=0.066281\n",
      "epoch 9/20   error=0.059400\n",
      "epoch 10/20   error=0.054284\n",
      "epoch 11/20   error=0.049268\n",
      "epoch 12/20   error=0.046270\n",
      "epoch 13/20   error=0.042640\n",
      "epoch 14/20   error=0.040235\n",
      "epoch 15/20   error=0.037943\n",
      "epoch 16/20   error=0.036226\n",
      "epoch 17/20   error=0.035045\n",
      "epoch 18/20   error=0.033750\n",
      "epoch 19/20   error=0.032382\n",
      "epoch 20/20   error=0.031627\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 70   0   3   3   1   8   6   2   1   2]\n",
      " [  1 116   1   1   0   1   1   1   0   0]\n",
      " [  0   3  75   3   0   0   2   4   3   4]\n",
      " [  2   2   5  76   1  13   0   8   6   2]\n",
      " [  1   0   6   1  67   1   5  10   1  11]\n",
      " [  0   0   3   5   1  65   3   1   5   3]\n",
      " [  0   0   5   0   2   4  82   3   2   1]\n",
      " [  0   1   2   0   3   2   0  71   1   6]\n",
      " [  1   6   7   5   1  13   3   2  58   5]\n",
      " [  1   0   2   3   9   1   0   8   1  73]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.753\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Parámetro regularizados\n",
    "lambda_reg = 0.001\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 128))\n",
    "model.add(L1RegularizationLayer(lambda_reg=0.01))  # Agregar capa L1\n",
    "model.add(NoiseLayer(noise_std=0.1))  # Agregar capa Noise\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(FCLayer(128, 64))\n",
    "model.add(L1RegularizationLayer(lambda_reg=0.01))  \n",
    "model.add(NoiseLayer(noise_std=0.1))  \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(64, 10))\n",
    "model.add(L1RegularizationLayer(lambda_reg=0.01))  \n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Asignamos función de pérdida\n",
    "model.use(bce, bce_prime)\n",
    "\n",
    "# Entrenamos el modelo con datos de entrenamiento\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.01)\n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
