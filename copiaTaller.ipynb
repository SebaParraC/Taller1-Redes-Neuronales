{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #generar números aleatorios\n",
    "import numpy as np #para trabajar con arrays y matrices\n",
    "from keras.datasets import mnist #conjunto de datos de imágenes de dígitos escritos a mano\n",
    "from keras.utils import to_categorical #\n",
    "from sklearn.preprocessing import MinMaxScaler #scalar los datos a un rango\n",
    "from sklearn.model_selection import train_test_split #dividir los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "# Cargar el dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# se crea una lista de tuplas con los elementos de x e y, luego se toma una muestra aleatoria de 2000 tuplas, se descomprime y con zip se vuelven a separar en elementos de x e y \n",
    "X, y = zip(*random.sample(list(zip(X_train, y_train)), 2000))\n",
    "\n",
    "# Sí necesitamos que la forma de X sea la de un vector, en lugar de una matriz. \n",
    "X, y = np.array(X, dtype='float64'), np.array(y, dtype='float64')\n",
    "X = np.reshape(X, (X.shape[0], -1))\n",
    "\n",
    "# Normalizamos Min-Max\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "# Dividimos la muestra en dos, una para entrenar y otra para testing, como tenemos \n",
    "# muestra de sobra nos damos el lujo de testear con la misma cantidad que entrenamos.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=123)\n",
    "\n",
    "# Necesitamos que y_train sea un valor categórico, en lugar de un dígito entero.\n",
    "y_train_value = y_train # Guardaremos y_train como valor para un observación más abajo.\n",
    "y_train = to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Clase base para Capa\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "# Clase para capas densas (fully connected)\n",
    "class FCLayer(Layer):\n",
    "    def __init__(self, input_size, output_size, lambda_reg=0):\n",
    "        # np.random.seed(1234)\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "        self.lambda_reg = lambda_reg  # Se agrega parámetro coeficiente lambda de regularización L2\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "\n",
    "        # Adicionamos acá el término de regularización L2 que castiga error en los pesos\n",
    "        weights_error += self.lambda_reg * self.weights\n",
    "\n",
    "        # Actualizar los parámetros\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error\n",
    "    \n",
    "# Clase para Capa de Activación. Junto con la capa densa forman perceptrones. \n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        return self.output\n",
    "        # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return self.activation_prime(self.input) * output_error\n",
    "    \n",
    "# Clase Red, conecta múltiples capas.\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "\n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set loss to use\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        if input_data.ndim == 1: # YA NO SIEMPRE VAMOS A RECIBIR ARREGLOS UNIDIMENSIONALES\n",
    "            input_data = np.array([[x] for x in input_data])\n",
    "        samples = len(input_data)\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "     # train the network\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        if x_train[0].ndim == 1: # YA NO SIEMPRE VAMOS A RECIBIR ARREGLOS UNIDIMENSIONALES\n",
    "            x_train = np.array([[x] for x in x_train])\n",
    "        samples = len(x_train)\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            \n",
    "            # Para usar en clasificación (con más de dos clases)\n",
    "            # calculamos el error promedio entre nodos de salida.\n",
    "            err = np.mean(err)\n",
    "            \n",
    "            # Imprimimos el error promedio de cada época, más que nada\n",
    "            # para seguimiento del aprendizaje. \n",
    "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "    # Funciones de Activación, y su correspondiente función derivada. \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sig = sigmoid(x)  # Calculamos sigmoid(x) para cada elemento del vector\n",
    "    return sig * (1 - sig)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    # Versión aproximada de ReLu', porque ReLu no es redivable en x=0\n",
    "    # La derivada de ReLU es 1 si x > 0, y 0 si x <= 0\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "# función Leaky ReLU\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    # Si x es positivo, devuelve x; si no, devuelve alpha * x\n",
    "    return np.where(x > 0, x, alpha * x)\n",
    "\n",
    "# Implementamos la derivada de Leaky ReLU\n",
    "def leaky_relu_prime(x, alpha=0.01):\n",
    "    # La derivada es 1 si x > 0, y alpha si x <= 0\n",
    "    return np.where(x > 0, 1, alpha)\n",
    "\n",
    "\n",
    "# Funciones de pérdida y su derivada. \n",
    "# Error Cuadrático\n",
    "def mse(y_true, y_hat):\n",
    "    return (y_true-y_hat)**2\n",
    "\n",
    "def mse_prime(y_true, y_hat):\n",
    "    return 2*(y_hat-y_true)\n",
    "\n",
    "# Mini batch generator\n",
    "\n",
    "def mini_batch_generator(X, y, batch_size):\n",
    "    batch_size = 32\n",
    "    num_samples = X.shape[0]\n",
    "    indices = np.arange(num_samples)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "# Entropía cruzada binaria\n",
    "import numpy as np\n",
    "\n",
    "# Implementamos la función de error BCE\n",
    "def bce(y_true, y_hat):\n",
    "    # Evitamos problemas de logaritmo aplicando un pequeño epsilon\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "    \n",
    "    return -(y_true * np.log(y_hat) + (1 - y_true) * np.log(1 - y_hat))\n",
    "\n",
    "# Implementamos la derivada de BCE\n",
    "def bce_prime(y_true, y_hat):\n",
    "    # Evitamos problemas de división por 0\n",
    "    epsilon = 1e-15\n",
    "    y_hat = np.clip(y_hat, epsilon, 1 - epsilon)\n",
    "\n",
    "    # Derivada de BCE con respecto a la predicción\n",
    "    return -(y_true / y_hat) + (1 - y_true) / (1 - y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y, batch_size=32):\n",
    "    n_samples, n_features = X.shape\n",
    "    self.weights = np.zeros(n_features)\n",
    "    self.bias = 0\n",
    "\n",
    "    for _ in range(self.n_iters):\n",
    "        # Barajado aleatorio del mini-lote\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = y[indices]\n",
    "\n",
    "        # Procesamiento del mini-lote\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            # Seleccionamos el mini-lote actual\n",
    "            X_batch = X_shuffled[i:i+batch_size]\n",
    "            y_batch = y_shuffled[i:i+batch_size]\n",
    "\n",
    "            # Calculamos la salida del modelo\n",
    "            linear_output = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_predicted = self._unit_step_function(linear_output)\n",
    "\n",
    "            # Calculamos el error\n",
    "            error = y_batch - y_predicted\n",
    "\n",
    "            # Calculamos el gradiente promedio del mini-lote\n",
    "            dw = np.mean(error * X_batch, axis=0)\n",
    "            db = np.mean(error)\n",
    "\n",
    "            # Realizamos el ajuste de pesos\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "        # Ajuste del último mini-lote\n",
    "        if i + batch_size > n_samples:\n",
    "            X_batch = X_shuffled[i:]\n",
    "            y_batch = y_shuffled[i:]\n",
    "\n",
    "            # Calculamos la salida del modelo\n",
    "            linear_output = np.dot(X_batch, self.weights) + self.bias\n",
    "            y_predicted = self._unit_step_function(linear_output)\n",
    "\n",
    "            # Calculamos el error\n",
    "            error = y_batch - y_predicted\n",
    "\n",
    "            # Calculamos el gradiente promedio del mini-lote\n",
    "            dw = np.mean(error * X_batch, axis=0)\n",
    "            db = np.mean(error)\n",
    "\n",
    "            # Realizamos el ajuste de pesos\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.max(input_data.reshape(-1, self.pool_size, self.pool_size), axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = np.zeros_like(self.input)\n",
    "        for i in range(self.pool_size):\n",
    "            for j in range(self.pool_size):\n",
    "                error[:, i::self.pool_size, j::self.pool_size] = output_error\n",
    "        return error\n",
    "\n",
    "class AveragePoolingLayer(Layer):\n",
    "    def __init__(self, pool_size):\n",
    "        self.pool_size = pool_size\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = np.mean(input_data.reshape(-1, self.pool_size, self.pool_size), axis=1)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = np.zeros_like(self.input)\n",
    "        for i in range(self.pool_size):\n",
    "            for j in range(self.pool_size):\n",
    "                error[:, i::self.pool_size, j::self.pool_size] = output_error / (self.pool_size ** 2)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseLayer(Layer):\n",
    "    def __init__(self, noise_std):\n",
    "        self.noise_std = noise_std\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = input_data + np.random.normal(0, self.noise_std, size=input_data.shape)\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        return output_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1RegularizationLayer(Layer):\n",
    "    def __init__(self, lambda_reg):\n",
    "        self.lambda_reg = lambda_reg\n",
    "\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = input_data\n",
    "        return self.output\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        error = output_error\n",
    "        for i in range(self.input.shape[1]):\n",
    "            if self.input[0, i] > 0:\n",
    "                error[0, i] += self.lambda_reg\n",
    "            elif self.input[0, i] < 0:\n",
    "                error[0, i] -= self.lambda_reg\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/20   error=0.266118\n",
      "epoch 2/20   error=0.170029\n",
      "epoch 3/20   error=0.133192\n",
      "epoch 4/20   error=0.111514\n",
      "epoch 5/20   error=0.097060\n",
      "epoch 6/20   error=0.087161\n",
      "epoch 7/20   error=0.079671\n",
      "epoch 8/20   error=0.074695\n",
      "epoch 9/20   error=0.070881\n",
      "epoch 10/20   error=0.068333\n",
      "epoch 11/20   error=0.065986\n",
      "epoch 12/20   error=0.064391\n",
      "epoch 13/20   error=0.063029\n",
      "epoch 14/20   error=0.061751\n",
      "epoch 15/20   error=0.060728\n",
      "epoch 16/20   error=0.059711\n",
      "epoch 17/20   error=0.059026\n",
      "epoch 18/20   error=0.058293\n",
      "epoch 19/20   error=0.057655\n",
      "epoch 20/20   error=0.056906\n",
      "MATRIZ DE CONFUSIÓN para modelo ANN\n",
      "[[ 96   0   0   0   0   1   1   1   1   3]\n",
      " [  0 142   0   0   1   0   0   0   0   0]\n",
      " [  2   0  80   2   5   0   7   3   1   0]\n",
      " [  0   1   2  82   0   4   1   1   1   2]\n",
      " [  2   1   0   0  80   0   0   0   0  11]\n",
      " [  5   1   0   0   3  68   3   0   0   0]\n",
      " [  0   1   1   0   1   0 100   0   1   0]\n",
      " [  1   3   0   0   3   0   0  89   0   9]\n",
      " [  1   4   1   4   0   1   1   0  60   8]\n",
      " [  1   1   0   0   5   0   0   4   1  85]] \n",
      "\n",
      "La exactitud de testeo del modelo ANN es: 0.882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "# Necesitamos identificar cuantos nodos tiene nuestra entrada, y eso depende del tamaño de X.\n",
    "entrada_dim = len(X_train[0])\n",
    "\n",
    "# Crear instancia de Network\n",
    "model = Network()\n",
    "\n",
    "# Agregamos capas al modelo\n",
    "model.add(FCLayer(entrada_dim, 128, lambda_reg=0.01))  # Agregar capa L1\n",
    "model.add(NoiseLayer(noise_std=0.1))  # Agregar capa Noise\n",
    "model.add(ActivationLayer(relu, relu_prime))\n",
    "model.add(FCLayer(128, 64, lambda_reg=0.01))  # Agregar capa L1\n",
    "model.add(NoiseLayer(noise_std=0.1))  # Agregar capa Noise\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "model.add(FCLayer(64, 10, lambda_reg=0.01))  # Agregar capa L1\n",
    "model.add(ActivationLayer(sigmoid, sigmoid_prime))\n",
    "\n",
    "# Asignamos función de pérdida\n",
    "model.use(bce, bce_prime)\n",
    "\n",
    "# Entrenamos el modelo con datos de entrenamiento\n",
    "model.fit(X_train, y_train, epochs=20, learning_rate=0.01)\n",
    "\n",
    "# Usamos el modelo para predecir sobre los datos de prueba (validación)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "# Transformamos la salida en un vector one-hot encoded, es decir 0s y un 1. \n",
    "for i in range(len(y_hat)):\n",
    "    y_hat[i] = np.argmax(y_hat[i][0])\n",
    "\n",
    "# Reportamos los resultados del modelo\n",
    "matriz_conf = confusion_matrix(y_test, y_hat)\n",
    "\n",
    "print('MATRIZ DE CONFUSIÓN para modelo ANN')\n",
    "print(matriz_conf,'\\n')\n",
    "print('La exactitud de testeo del modelo ANN es: {:.3f}'.format(accuracy_score(y_test,y_hat)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
